{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time, random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "def get_weights(name,shape):\n",
    "    with tf.variable_scope(\"weights\", reuse=tf.AUTO_REUSE): \n",
    "        return tf.get_variable(name=name,shape=shape,initializer = tf.contrib.layers.xavier_initializer(uniform=False),regularizer = tf.contrib.layers.l2_regularizer(tf.constant(0.001, dtype=tf.float32)))\n",
    "    \n",
    "def get_bias(name,shape):\n",
    "    with tf.variable_scope(\"bias\", reuse=tf.AUTO_REUSE):\n",
    "        return tf.get_variable(name=name,shape=shape,initializer = tf.zeros_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layers\n",
    "\n",
    "def conv2d(inp,name,kshape,s):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        kernel = get_weights('weights',shape=kshape)\n",
    "        conv = tf.nn.conv2d(inp,kernel,[1,s,s,1],'SAME')\n",
    "        bias = get_bias('biases',shape=kshape[3])\n",
    "        preact = tf.nn.bias_add(conv,bias)\n",
    "        convlayer = tf.nn.relu(preact,name=scope.name)\n",
    "    return convlayer\n",
    "\n",
    "def maxpool(inp,name,k,s):\n",
    "    return tf.nn.max_pool(inp,ksize=[1,k,k,1],strides=[1,s,s,1],padding='SAME',name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Losses and Accuracy\n",
    "\n",
    "def loss(logits,labels):\n",
    "    labels = tf.reshape(tf.cast(labels,tf.int64),[-1])\n",
    "    #print labels.get_shape().as_list(),logits.get_shape().as_list()\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    total_loss = tf.add(tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)),cross_entropy_mean,name='total_loss')\n",
    "    return total_loss\n",
    "\n",
    "def top_1_acc(logits,true_labels):\n",
    "    pred_labels = tf.argmax(logits,1)\n",
    "    true_labels = tf.cast(true_labels,tf.int64)\n",
    "    #print pred_labels.get_shape().as_list(),true_labels\n",
    "    correct_pred = tf.cast(tf.equal(pred_labels, true_labels), tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def top_5_acc(logits,true_labels):\n",
    "    true_labels = tf.cast(true_labels,tf.int64)\n",
    "    return tf.reduce_mean(tf.cast(tf.nn.in_top_k(logits, true_labels, k=5,name='top_5_acc'), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "\n",
    "def get_new_size():\n",
    "    new_size = 96 + random.choice([24,16,0])\n",
    "    return [new_size,new_size]\n",
    "\n",
    "def get_random_augmentation_combinations(length):\n",
    "    out = [False,True]\n",
    "    return [random.choice(out) for i in range(length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Image data\n",
    "\n",
    "def get_all_images(img_file):\n",
    "    images = np.fromfile(img_file,dtype=np.uint8).astype(np.float32)\n",
    "    images = np.reshape(images,(-1,3,96,96))\n",
    "    images = np.transpose(images,(0,3,2,1))\n",
    "    print('Normalizing Inputs...')\n",
    "    rmean = np.mean(images[:,:,:,0])\n",
    "    gmean = np.mean(images[:,:,:,1])\n",
    "    bmean = np.mean(images[:,:,:,2])\n",
    "    images[:,:,:,0] = (images[:,:,:,0] - rmean)#/rstd\n",
    "    images[:,:,:,1] = (images[:,:,:,1] - gmean)#/gstd\n",
    "    images[:,:,:,2] = (images[:,:,:,2] - bmean)#/bstd\n",
    "    print ('R_mean:',rmean,'G_mean:',gmean,'B_mean:',bmean)\n",
    "    return images,rmean,gmean,bmean\n",
    "\n",
    "def get_all_labels(label_file):\n",
    "    labels = np.fromfile(label_file,dtype=np.uint8)\n",
    "    #print labels.shape\n",
    "    return labels\n",
    "\n",
    "def get_test_images(img_file,rmean,gmean,bmean):\n",
    "    images = np.fromfile(img_file,dtype=np.uint8).astype(np.float32)\n",
    "    images = np.reshape(images,(-1,3,96,96))\n",
    "    images = np.transpose(images,(0,3,2,1))\n",
    "    print('Normalizing Validation Images...')\n",
    "    images[:,:,:,0] = (images[:,:,:,0] - rmean)#/rstd\n",
    "    images[:,:,:,1] = (images[:,:,:,1] - gmean)#/gstd\n",
    "    images[:,:,:,2] = (images[:,:,:,2] - bmean)#/bstd\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "#Getting the dataset\n",
    "\n",
    "\n",
    "print('Getting the data...')\n",
    "train_data_path = '/floyd/input/stl10_binary/train_X.bin' #/media/siladittya/fdc481ce-9355-46a9-b381-9001613e3422/siladittya/StudyMaterials/ISI/code/ds/stl10_binary\n",
    "train_label_path = '/floyd/input/stl10_binary/train_y.bin'\n",
    "\n",
    "train_img_file = open(train_data_path,'rb')\n",
    "train_label_file = open(train_label_path,'rb')\n",
    "\n",
    "train_x,rmean,gmean,bmean = get_all_images(train_img_file)\n",
    "train_y = get_all_labels(train_label_file)\n",
    "train_y = train_y - 1\n",
    "#Getting Validation Dataset\n",
    "test_img_path = '/floyd/input/stl10_binary/test_X.bin'\n",
    "test_label_path = '/floyd/input/stl10_binary/test_y.bin'\n",
    "\n",
    "test_img_file = open(test_img_path,'rb')\n",
    "test_label_file = open(test_label_path,'rb')\n",
    "\n",
    "test_x = get_test_images(test_img_file,rmean,gmean,bmean)\n",
    "test_y = get_all_labels(test_label_file)\n",
    "test_y = test_y - 1\n",
    "print('Getting Validation set from Test set...')\n",
    "val_x = test_x[1:2000]\n",
    "val_y = test_y[1:2000]\n",
    "\n",
    "train_x = np.append(train_x,test_x[3000:],axis=0)\n",
    "train_y = np.append(train_y,test_y[3000:],axis=0)\n",
    "print(\"Training Set size = \",train_x.shape)\n",
    "\n",
    "test_x = test_x[2000:3000]\n",
    "test_y = test_y[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholders and constants\n",
    "\n",
    "index = np.arange(train_x.shape[0])\n",
    "\n",
    "#........ This part will used to get training data for each epoch during training\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "numiter = 200\n",
    "ne = 0\n",
    "\n",
    "feed_images = tf.placeholder(tf.float32,shape=(None,96,96,3))\n",
    "feed_labels = tf.placeholder(tf.float32,shape=(None,))\n",
    "lr = tf.placeholder(tf.float32,shape=())\n",
    "keep_prob = tf.placeholder(tf.float32,shape=())\n",
    "aug_img = tf.placeholder(tf.float32,shape=(96,96,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG 13 ConvNet\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    conv1 = conv2d(feed_images,'conv1',[3,3,3,64],1)\n",
    "    conv2 = conv2d(conv1,'conv2',[3,3,64,64],1)\n",
    "    pool1 = maxpool(conv2,'pool1',2,2)\n",
    "    #size = [N,48,48,64]\n",
    "    conv3 = conv2d(pool1,'conv3',[3,3,64,128],1)\n",
    "    conv4 = conv2d(conv3,'conv4',[3,3,128,128],1)\n",
    "    pool2 = maxpool(conv4,'pool2',2,2)\n",
    "    #size = [N,24,24,128]\n",
    "    conv5 = conv2d(pool2,'conv5',[3,3,128,256],1)\n",
    "    conv6 = conv2d(conv5,'conv6',[3,3,256,256],1)\n",
    "    pool3 = maxpool(conv6,'pool3',2,2)\n",
    "    #size = [N,12,12,256]\n",
    "    conv7 = conv2d(pool3,'conv7',[3,3,256,512],1)\n",
    "    conv8 = conv2d(conv7,'conv8',[3,3,512,512],1)\n",
    "    pool4 = maxpool(conv8,'pool4',2,2)\n",
    "    #size = [N,6,6,512]\n",
    "    conv9 = conv2d(pool4,'conv9',[3,3,512,512],1)\n",
    "    conv10 = conv2d(conv9,'conv10',[3,3,512,512],1)\n",
    "    pool5 = maxpool(conv10,'pool5',2,2)\n",
    "    #size = [N,3,3,512]\n",
    "    flattened_pool5 = tf.contrib.layers.flatten(pool5)\n",
    "    fc1 = tf.contrib.layers.fully_connected(flattened_pool5,512,weights_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.001, dtype=tf.float32)))\n",
    "    dropout1 = tf.nn.dropout(fc1,keep_prob)\n",
    "    fc2 = tf.contrib.layers.fully_connected(dropout1,512,weights_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.001, dtype=tf.float32)))\n",
    "    dropout2 = tf.nn.dropout(fc2,keep_prob)\n",
    "    logits = tf.contrib.layers.fully_connected(dropout2,10,activation_fn=None,weights_regularizer=tf.contrib.layers.l2_regularizer(tf.constant(0.001, dtype=tf.float32)))\n",
    "\n",
    "    cost = loss(logits,feed_labels)\n",
    "\n",
    "    opt_mom = tf.train.AdamOptimizer(learning_rate=0.0001)#,momentum=0.9)\n",
    "    opt = opt_mom.minimize(cost)\n",
    "\n",
    "    acc = top_1_acc(logits,feed_labels)\n",
    "#Defined outside gpu0 device since tf.nn.in_top_k is not supported for gpu kernel\n",
    "valacc = top_5_acc(logits,feed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "\n",
    "img_scale_crop = tf.random_crop(tf.image.resize_images(aug_img,get_new_size()),[96,96,3])\n",
    "img_rand_flip_lr = tf.image.random_flip_left_right(aug_img)\n",
    "img_rand_flip_ud = tf.image.random_flip_up_down(aug_img)\n",
    "img_con = tf.image.random_contrast(aug_img,0.01,0.25)\n",
    "img_br = tf.image.random_brightness(aug_img,0.25)\n",
    "img_rot = tf.contrib.image.rotate(aug_img,0.785*(-0.5+random.random()),interpolation='BILINEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "ne = 0\n",
    "val_x = val_x[:1000]\n",
    "val_y = val_y[:1000]\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tl=[]\n",
    "vl=[]\n",
    "ta=[] #Top-1 Training accuracy\n",
    "ta5 = [] #Top-5 Training accuracy\n",
    "va=[] #Top-1 Validation accuracy\n",
    "va5 = [] #Top-5 Validation accuracy\n",
    "\n",
    "while(ne<30):\n",
    "    stime = time.time()\n",
    "    print 'Epoch::',ne+1,'...'\n",
    "    \n",
    "    #Shuffling the Dataset\n",
    "    if ne != 0:\n",
    "        np.random.shuffle(index)\n",
    "        train_x = train_x[index]\n",
    "        train_y = train_y[index]\n",
    "    \n",
    "    for niter in range(numiter):\n",
    "    \n",
    "        if (niter+1)%50==0:\n",
    "            print 'iter..',niter+1\n",
    "        \n",
    "        #Getting next Batch\n",
    "        offset = niter*batch_size\n",
    "        x_iter, y_iter = np.array(train_x[offset:offset+batch_size,:,:,:]), np.array(train_y[offset:offset+batch_size])\n",
    "        \n",
    "        #Data Augmentation\n",
    "        for n in range(batch_size):\n",
    "            args = get_random_augmentation_combinations(3)\n",
    "            if args[0]:\n",
    "                x_iter[n] = sess.run(img_scale_crop,feed_dict={aug_img:x_iter[n]})\n",
    "            if args[1]:\n",
    "                x_iter[n] = sess.run(img_rand_flip_lr,feed_dict={aug_img:x_iter[n]})\n",
    "            if args[2]: \n",
    "                x_iter[n] = sess.run(img_rand_flip_ud,feed_dict={aug_img:x_iter[n]})\n",
    "                \n",
    "        feed_trdict={feed_images:x_iter,feed_labels:y_iter,keep_prob:0.55}#,lr:0.01\n",
    "            \n",
    "        #Train the optimizer\n",
    "        sess.run(opt,feed_dict=feed_trdict)\n",
    "\n",
    "    #Calculate accuracy of Training set\n",
    "    cc = sess.run(cost,feed_dict=feed_trdict)\n",
    "    tr_acc = sess.run(acc,feed_dict = {feed_images:x_iter,feed_labels:y_iter,keep_prob:1.0})\n",
    "    top5_tr_acc = sess.run(valacc,feed_dict = {feed_images:x_iter,feed_labels:y_iter,keep_prob:1.0})\n",
    "    ta.append(tr_acc)\n",
    "    ta5.append(top5_tr_acc)\n",
    "    tl.append(cc)\n",
    "    \n",
    "    #Calculate accuracy of Validation set\n",
    "    val_loss = sess.run(cost,feed_dict = {feed_images:val_x,feed_labels:val_y,keep_prob:1.0})\n",
    "    top5_val_acc = sess.run(valacc,feed_dict = {feed_images:val_x,feed_labels:val_y,keep_prob:1.0})\n",
    "    top1_val_acc = sess.run(acc,feed_dict = {feed_images:val_x,feed_labels:val_y,keep_prob:1.0})\n",
    "    va.append(top1_val_acc)\n",
    "    va5.append(top5_val_acc)\n",
    "    vl.append(val_loss)\n",
    "    \n",
    "    #print 'Epoch..',ne+1,'...'\n",
    "    print 'Training accuracy-> Top-1::',tr_acc*100,'%','Top-5:: ',top5_tr_acc*100,'%',' Training cost::',cc\n",
    "    print 'Top-1 Validation accuracy::',top1_val_acc*100,'Top-5 Val Accuracy:: ',top5_val_acc*100,'%',' Val loss: ',val_loss\n",
    "    print 'Time reqd.::',(time.time()-stime)/60,'mins...'\n",
    "\n",
    "    ne+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_meta_graph_and_variables(sess, [\"EVALUATING\"])\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ta)\n",
    "plt.plot(va)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ta5)\n",
    "plt.plot(va5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tl)\n",
    "plt.plot(vl)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libmr as mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------META-RECOGNITION PART 1---------------------------------------#\n",
    "\n",
    "#penult_layer = tf.random_uniform(shape=[1,10],maxval=1.0)\n",
    "softmax_layer = tf.nn.softmax(logits)\n",
    "pred_label = tf.argmax(softmax_layer,1)\n",
    "_,top5_pred = tf.nn.top_k(softmax_layer, k=5)\n",
    "\n",
    "mav = {}\n",
    "av = {}\n",
    "\n",
    "mav5 = {}\n",
    "av5 = {}\n",
    "\n",
    "for i in xrange(10):\n",
    "    av[i] = np.array([]).reshape((0,10))\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    print 'Starting processing activation vectors...'\n",
    "    stime = time.time()\n",
    "    stime1=stime\n",
    "    for i1 in xrange(10000):\n",
    "        train_img = train_x[i1].reshape((1,96,96,3))\n",
    "        true_label = train_y[i1]\n",
    "        if (i1+1)%1000==0:\n",
    "            print \"Analysing\",\".\"*int(i1/1000)\n",
    "            print 'Time for ',i1+1,': ',time.time()-stime,' secs...'\n",
    "            stime = time.time()\n",
    "        #For Top-1 classification\n",
    "        penult,pred_y,top5pred = sess.run([logits,pred_label,top5_pred],feed_dict={feed_images:train_img,keep_prob:1.0})\n",
    "        pred_y = pred_y[0]\n",
    "        #stime = time.time()\n",
    "        #print np.array(penult).shape\n",
    "        \n",
    "        if pred_y==true_label:\n",
    "            av[true_label] = np.append(av[true_label],np.array(penult),axis=0)\n",
    "            #print av[true_label].shape, pred_y, true_label\n",
    "        #print'.........................', av\n",
    "    print 'Completion time:: ', time.time()-stime1,'secs...'\n",
    "    #Calculating the Mean activation Vector\n",
    "    print \"Calculating the Mean activation Vector...\"\n",
    "    for i in xrange(10):\n",
    "        mav[i] = np.mean(av[i],axis=0)\n",
    "        #print mav[i]\n",
    "    \n",
    "    dist_from_respmean = {}\n",
    "    \n",
    "    for i in xrange(10):\n",
    "        dist_from_respmean[i] = []\n",
    "    \n",
    "    #dist_from_respmean5 = dist_from_respmean\n",
    "    #Calculating the largest of the Distances\n",
    "    print \"Calculating the largest of the Distances...\"\n",
    "    for i in xrange(10):\n",
    "        n = av[i].shape[0]\n",
    "        #print n\n",
    "        for j in xrange(n):\n",
    "            penult = av[i][j]\n",
    "            dist_from_respmean[i].append(np.sqrt(np.sum(np.square(np.subtract(penult,mav[true_label])))))\n",
    "\n",
    "\n",
    "    #Sorting the distances\n",
    "    print \"Sorting the distances\"\n",
    "    for i in xrange(10):\n",
    "        dist_from_respmean[i] = sorted(dist_from_respmean[i])\n",
    "    \n",
    "    \n",
    "    #Weibull tail fitting\n",
    "    models = []\n",
    "    print \"Weibull Tail Fitting...\"\n",
    "    for i in xrange(10):\n",
    "        meta = mr.MR()\n",
    "        meta.fit_high(dist_from_respmean[i][-50:],50)\n",
    "        models.append(meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------META-RECOGNITION PART 2-------------------------------\n",
    "for n in xrange(100,300):\n",
    "    input_img = unlab_x[n].reshape((1,96,96,3))\n",
    "    actvec = sess.run(logits,feed_dict = {feed_images:input_img,keep_prob:1.0})\n",
    "    actvec = np.array(actvec[0])\n",
    "    #print 'Activation Vector: ',actvec,'\\n'\n",
    "    #Calculating softmax values with unkown as 0.0\n",
    "    softmax_val = np.exp(actvec)/np.sum(np.exp(actvec))\n",
    "    softmax_val = np.insert(softmax_val,0,0.0)\n",
    "    print 'Softmax Values: ',softmax_val\n",
    "    #..........................Sorting the activation vector\n",
    "    sorted_actvec = sorted(actvec)\n",
    "    sorting_order = actvec.argsort()\n",
    "    #print 'Sorted: ',sorted_actvec,'\\n'\n",
    "    #..................Calculatng alpha weight and dist from mean of each class\n",
    "    alpha_weight = []\n",
    "    dist_from_means = []\n",
    "    for i in xrange(1,11):\n",
    "        alpha_weight.append((11-i)/10.0)\n",
    "        dist_from_means.append(np.sqrt(np.sum(np.square(np.subtract(actvec,mav[i-1])))))\n",
    "    #print 'Distance from means:: ',dist_from_means,'\\n'\n",
    "    #..........................Calculating weilbull CDF score\n",
    "    wscore = []\n",
    "    #Unknown is taken as 0th label\n",
    "    for i in xrange(1,11):\n",
    "        wscore.append(1 - alpha_weight[i-1]*(1-models[i-1].w_score(dist_from_means[i-1])))\n",
    "        \n",
    "    #........................Calculating revised activation vector\n",
    "    wscore = np.array(wscore)\n",
    "    #print wscore\n",
    "    revised_act = np.zeros((1,11))\n",
    "    for i in xrange(1,11):\n",
    "        revised_act[:,i] = actvec[i-1]*wscore[i-1]\n",
    "    \n",
    "    #Calculating activation vector value for 0th label i.e. unknown label\n",
    "    revised_act[:,0] = np.sum(np.multiply(actvec,1 - wscore))\n",
    "    #print revised_act\n",
    "    #.........................Compute Openmax probabilities\n",
    "    openmax_val = np.exp(revised_act)/np.sum(np.exp(revised_act))\n",
    "    print 'Openmax valuess: ',openmax_val[0]\n",
    "    #........................Checking the predictions\n",
    "    top1_pred = np.argmax(np.array(openmax_val[0]))\n",
    "    \n",
    "    top5_preds = np.array(openmax_val[0]).argsort()[-5:][::-1]\n",
    "    \n",
    "    plt.imshow((unlab_x[n]+np.reshape(np.array([rmean,gmean,bmean]),(1,1,3)))/255)\n",
    "    plt.show()\n",
    "    print '0->Unknown|1->Aeroplane|2->Bird|3->Car|4->Cat|5->Deer|6->Dog|7->Horse|8->Monkey|9->Ship|10->Truck'\n",
    "    if top1_pred == 0 or openmax_val[0][top1_pred] < 0.5:\n",
    "        print 'Prediction: Unknown\\n'\n",
    "        print 'Unrevised Values: ',np.array(softmax_val).argsort()[-5:][::-1]\n",
    "        print 'Revised Top 5:' ,top5_preds\n",
    "    else:\n",
    "        print 'Prediction: Known'\n",
    "        print 'Unrevised Values: ',np.array(softmax_val).argsort()[-5:][::-1]\n",
    "        print 'Revised Top 5:' ,top5_preds\n",
    "    print '..........................................................................................'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
